import pandas as pd
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedShuffleSplit
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from Score import Score
from config import INPUT_CSV_FILENAME, RANDOM_STATE, FOLDS

plt.style.use('ggplot')

# Load csv generated by DatasetGenerator directory
data = pd.read_csv('../DatasetGenerator/DataSets/' + INPUT_CSV_FILENAME)

# Separate features and label
X = data.iloc[:, :-1].values
# X = np.concatenate([X, X])
y = data.iloc[:, 20].values
# y = np.concatenate([y, y])

# Separate data into outer training and testing
X_outer_train, X_outer_test, y_outer_train, y_outer_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)


# Using stratified sampling on training set
sss = StratifiedShuffleSplit(n_splits=FOLDS, test_size=0.33, random_state=RANDOM_STATE)

# choose C between 0 and 1
clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1)




accuracy = 0
precision = 0
f1 = 0
recall = 0
# 10 fold cross validation
for train_index, test_index in sss.split(X_outer_train, y_outer_train):
    # Create model
    model = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=RANDOM_STATE)
    # Extract training and testing data
    X_inner_train, X_inner_test = X_outer_train[train_index], X_outer_train[test_index]
    y_inner_train, y_inner_test = y_outer_train[train_index], y_outer_train[test_index]
    # train the model and extract scores
    model.fit(X_inner_train, y_inner_train)
    prediction = model.predict(X_inner_test)
    accuracy += accuracy_score(y_inner_test, prediction)
    precision += precision_score(y_inner_test, prediction,
                                     zero_division=0, average="weighted")
    f1 += f1_score(y_inner_test, prediction, zero_division=0, average="weighted")
    recall += recall_score(y_inner_test, prediction, zero_division=0, average="weighted")
        
# Computer average scores from cross fold
accuracy = accuracy / FOLDS
precision = precision / FOLDS
f1 = f1 / FOLDS
recall = recall / FOLDS


# Get best C based off best f score
print("Cross Validated Results:")
print("Accuracy: ", accuracy * 100, "%")
print("Precision: ", precision * 100, "%")
print("Recall Score: ", recall)
print("F1 Score: ", f1)
print("Now trained on full dataset...")
# Train best model on whole dataset and show classification report
bestModel = MLPClassifier(solver='sgd', hidden_layer_sizes=(100, 100), random_state=RANDOM_STATE)
bestModel.fit(X_outer_train,y_outer_train)
bestPrediction = bestModel.predict(X_outer_test)
print(classification_report(y_outer_test,bestPrediction,zero_division=0))
print("Accuracy: ", accuracy_score(y_outer_test,bestPrediction) * 100, "%")
print("Precision: ", precision_score(y_outer_test,bestPrediction,zero_division=0,average="weighted") * 100, "%")
print("Recall Score: ", recall_score(y_outer_test,bestPrediction,zero_division=0,average="weighted"))
print("F1 Score: ", f1_score(y_outer_test,bestPrediction,zero_division=0,average="weighted"))